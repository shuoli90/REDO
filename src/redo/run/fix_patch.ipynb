{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "os.environ['ENVROOT'] = '/home/ubuntu/mnt/agent/amazon-Q/NGDEBirds/NGDEBirdsScienceTransforms/'\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "from typing import DefaultDict\n",
    "from birds_transforms.utils import (\n",
    "    find_intervals, \n",
    "    identify_code_location, \n",
    "    prompt_anthropic,\n",
    "    prompt_openai,\n",
    "    extract_tag,\n",
    "    extract_tag_attributes,\n",
    "    extract_tag_list,\n",
    ")\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from git import Repo\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "OPUS = \"claude-3-opus-20240229\"\n",
    "OPUS_BR = \"anthropic.claude-3-opus-20240229-v1:0\"\n",
    "HAIKU = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "SONNET = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "SONNET_3_5 = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "def extract_modified_lines(patch):\n",
    "    file_lines = DefaultDict(list)\n",
    "    # file_lines = {}\n",
    "    for diff in patch.split(\"--- a/\"):\n",
    "        if not diff.strip():\n",
    "            continue\n",
    "        diff_file = diff.splitlines()[0]\n",
    "        for hunk in diff.split(\"@@ -\")[1:]:\n",
    "            if not hunk.strip():\n",
    "                continue\n",
    "            try:\n",
    "                start_line, num_lines = hunk.split(\" \")[0].split(\",\")\n",
    "            except Exception as e:\n",
    "                print(\"Failed to extract from hunk in patch\")\n",
    "                continue\n",
    "            start_line, num_lines = int(start_line), int(num_lines)\n",
    "            offset = 0\n",
    "            previous_line_type = None\n",
    "            for line in hunk.splitlines()[1:]:\n",
    "                if line.startswith(\"-\"):\n",
    "                    file_lines[diff_file].append(start_line + offset)\n",
    "                    # file_lines.add(f\"{diff_file} | line number {start_line + offset}\")\n",
    "                    offset += 1\n",
    "                    previous_line_type = \"remove\"\n",
    "                elif line.startswith(\" \"):\n",
    "                    offset += 1\n",
    "                    previous_line_type = None\n",
    "                elif line.startswith(\"+\") and previous_line_type is None:\n",
    "                    # file_lines.add(f\"{diff_file} | line number {start_line + offset - 1}\")\n",
    "                    file_lines[diff_file].append(start_line + offset - 1)\n",
    "                    previous_line_type = \"add\"\n",
    "\n",
    "    for diff_file in file_lines:\n",
    "        file_lines[diff_file] = find_intervals(file_lines[diff_file])\n",
    "\n",
    "    return file_lines\n",
    "\n",
    "# setup repo\n",
    "def setup_repo(x, repo_root='~'):\n",
    "    \"\"\"Checks out the specified commit of a repository for the given input.\n",
    "\n",
    "    Clones the repository if not already present and checks out the specified commit. A\n",
    "    temporary copy of the repository at the specified commit is prepared for further use.\n",
    "\n",
    "    Args:\n",
    "        x: Input data containing 'repo' (GitHub repo path) and 'base_commit' (commit hash).\n",
    "\n",
    "    Returns:\n",
    "        The input data augmented with 'repo_dir' (temporary directory of the repo) and\n",
    "            'repo_url'.\n",
    "    \"\"\"\n",
    "    repo_url = f\"https://github.com/{x['repo']}.git\"\n",
    "\n",
    "    # Create base repo directory if it doesn't exist\n",
    "    repo_root = (\n",
    "        Path(repo_root).expanduser().resolve() if repo_root else Path.home() / \"repos\"\n",
    "    )\n",
    "    base_repo_dir = repo_root / x[\"repo\"].split(\"/\")[-1]\n",
    "    if not base_repo_dir.is_dir() or not (base_repo_dir / \".git\").exists():\n",
    "        if base_repo_dir.exists():\n",
    "            shutil.rmtree(base_repo_dir)\n",
    "        base_repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "        Repo.clone_from(repo_url, base_repo_dir).git.checkout(x[\"base_commit\"])\n",
    "\n",
    "    # Copy base repo to temporary directory\n",
    "    repo_dir = Path(\"/tmp\") / str(time.time()) / x[\"repo\"].split(\"/\")[-1]\n",
    "    if repo_dir.exists():\n",
    "        shutil.rmtree(repo_dir)\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copytree(base_repo_dir, repo_dir, dirs_exist_ok=True)\n",
    "\n",
    "    # Checkout required commit\n",
    "    repo = Repo(repo_dir)\n",
    "    git = repo.git\n",
    "    git.fetch(\"--all\")\n",
    "    git.reset(\"--hard\")\n",
    "    git.clean(\"-f\", \"-d\")\n",
    "    git.checkout(x[\"base_commit\"])\n",
    "    shutil.rmtree(repo_dir / \".git\")\n",
    "    return repo_dir\n",
    "\n",
    "\n",
    "def remove_repo(repo_dir):\n",
    "    if repo_dir.is_dir():\n",
    "        shutil.rmtree(repo_dir, ignore_errors=False)\n",
    "\n",
    "def read_python_script(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            script_body = file.read()\n",
    "        return script_body\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\"\n",
    "    except IOError:\n",
    "        return \"An error occurred while reading the file.\"\n",
    "\n",
    "def apply_patch(patch_path):\n",
    "    command = ['git', 'apply', str(patch_path)]\n",
    "    result = subprocess.run(command)\n",
    "\n",
    "def revert_patch(patch_path):\n",
    "    command = ['git', 'apply', '-R', str(patch_path)]\n",
    "    result = subprocess.run(command)\n",
    "\n",
    "from collections import Counter\n",
    "def majority_vote(lst):\n",
    "    count = Counter(lst)\n",
    "    majority_count = len(lst) // 2 + 1\n",
    "    \n",
    "    for elem, freq in count.items():\n",
    "        if freq >= majority_count:\n",
    "            return elem\n",
    "    \n",
    "    return None  # No majority element\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_modified_files_and_line_numbers(diff):\n",
    "    \"\"\"\n",
    "    Extract modified files and the corresponding line numbers of newly modified code from a git diff output.\n",
    "\n",
    "    Args:\n",
    "        diff (str): The git diff output as a string.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with file names as keys and lists of modified line numbers as values.\n",
    "    \"\"\"\n",
    "    file_line_numbers = defaultdict(list)\n",
    "    current_file = None\n",
    "    in_diff = False\n",
    "    current_line_number = 0\n",
    "    \n",
    "    # Process the diff line by line\n",
    "    for line in diff.splitlines():\n",
    "        if line.startswith('diff --git'):\n",
    "            # New file in the diff\n",
    "            current_file = re.search(r'a\\/(.*?)\\s', line).group(1)\n",
    "        elif line.startswith('@@'):\n",
    "            # Extract the starting line number from the diff chunk header\n",
    "            match = re.search(r'\\+(\\d+)', line)\n",
    "            if match:\n",
    "                hunk_start_line = int(match.group(1))\n",
    "                current_line_number = hunk_start_line\n",
    "            in_diff = True\n",
    "        elif in_diff:\n",
    "            if line.startswith('+') and not line.startswith('+++'):\n",
    "                # New line added, store the line number\n",
    "                file_line_numbers[current_file].append(current_line_number)\n",
    "                current_line_number += 1\n",
    "            elif line.startswith('-'):\n",
    "                # A line that was removed, do not increase line count\n",
    "                continue\n",
    "            elif line.startswith(' '):\n",
    "                # Context line, increase line count\n",
    "                current_line_number += 1\n",
    "            else:\n",
    "                # This marks the end of the diff chunk\n",
    "                in_diff = False\n",
    "\n",
    "    return file_line_numbers\n",
    "    \n",
    "def augment_intervals(intervals, total_lines, context=10):\n",
    "    augmented_intervals = []\n",
    "    \n",
    "    for start, end in intervals:\n",
    "        # Calculate augmented start and end with context\n",
    "        augmented_start = max(1, start - context)\n",
    "        augmented_end = min(total_lines, end + context)\n",
    "        augmented_intervals.append((augmented_start, augmented_end))\n",
    "    \n",
    "    return augmented_intervals\n",
    "\n",
    "def merge_line_numbers(line_numbers):\n",
    "    if not line_numbers:\n",
    "        return []\n",
    "\n",
    "    line_numbers.sort()\n",
    "    \n",
    "    ranges = []\n",
    "    start = end = line_numbers[0]\n",
    "\n",
    "    for num in line_numbers[1:]:\n",
    "        if num == end + 1:\n",
    "            end = num\n",
    "        else:\n",
    "            ranges.append((start, end))\n",
    "            start = end = num\n",
    "\n",
    "    ranges.append((start, end))\n",
    "    return ranges\n",
    "\n",
    "def extract_code_by_intervals(code_lines, intervals):\n",
    "    extracted_chunks = []\n",
    "    for start, end in intervals:\n",
    "        chunk = code_lines[start-1:end]  # Convert to 0-based index\n",
    "        extracted_chunks.append(\"\\n\".join(chunk))\n",
    "    return extracted_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in swe-bench dataset\n",
    "swebench_lite = datasets.load_dataset(\"princeton-nlp/SWE-bench_Lite\", split='test')\n",
    "swebench_lite_df = pd.DataFrame.from_dict(swebench_lite, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'autocoderover'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eval_autocoderover_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmethod\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_results.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      3\u001b[0m all_instances \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eval_autocoderover_results.json'"
     ]
    }
   ],
   "source": [
    "with open(f'eval_{method}_results.json', 'r') as f:\n",
    "    eval_results = json.load(f)\n",
    "all_instances = []\n",
    "for key in eval_results:\n",
    "    all_instances.extend(eval_results[key])\n",
    "all_instances = list(set(all_instances))\n",
    "with open(f'predicted_{method}_results.json', 'r') as f:\n",
    "    static_predicted_results = json.load(f)\n",
    "static_predicted_all = []\n",
    "for key in static_predicted_results:\n",
    "    if 'predicted' in key:\n",
    "        static_predicted_all.extend(static_predicted_results[key])\n",
    "static_predicted_results_pyrightm = list(set(static_predicted_all))\n",
    "if method == 'autocoderover':\n",
    "    prediction_path = \"/home/ubuntu/mnt/agent/experiments/evaluation/lite/20240621_autocoderover-v20240620/all_preds.jsonl\"\n",
    "elif method == 'aider':\n",
    "    prediction_path = \"/home/ubuntu/mnt/agent/experiments/evaluation/lite/20240523_aider/all_preds.jsonl\"\n",
    "elif method == 'codestory-mixed':\n",
    "    prediction_path = \"/home/ubuntu/mnt/agent/experiments/evaluation/lite/20240702_codestory_aide_mixed/all_preds.jsonl\"\n",
    "elif method == 'Demo':\n",
    "    prediction_path = \"/home/ubuntu/mnt/agent/experiments/evaluation/lite/20240627_abanteai_mentatbot_gpt4o/all_preds.jsonl\"\n",
    "elif method == 'droid':\n",
    "    prediction_path = \"/home/ubuntu/mnt/agent/experiments/evaluation/lite/20240617_factory_code_droid/all_preds.jsonl\"\n",
    "elif method == 'opus_func_margin':\n",
    "    prediction_path = \"~/predictions/opus_func_margin.jsonl\"\n",
    "elif method == 'lingma':\n",
    "    prediction_path = '/home/ubuntu/mnt/agent/experiments/evaluation/lite/20240622_Lingma_Agent/all_preds.jsonl'\n",
    "elif method == 'marscode':\n",
    "    prediction_path = '/home/ubuntu/mnt/agent/experiments/evaluation/lite/20240723_marscode-agent-dev/all_preds.jsonl'\n",
    "else:\n",
    "    raise ValueError(\"Invalid method\")\n",
    "preds = pd.read_json(prediction_path, lines=True)\n",
    "instances = preds['instance_id'].to_list()\n",
    "\n",
    "with open(f'predicted_{method}_results_pylint.json', 'r') as f:\n",
    "    static_predicted_pylint = json.load(f)\n",
    "static_predicted_all_pylint = []\n",
    "for key in static_predicted_pylint:\n",
    "    if 'predicted' in key:\n",
    "        static_predicted_all_pylint.extend(static_predicted_pylint[key])\n",
    "static_predicted_results_pylint = list(set(static_predicted_all_pylint))\n",
    "static_predicted_pylint_aggressive = []\n",
    "for instance_id, modified_issues in zip(instances, static_predicted_pylint['original_issues_lsit']):\n",
    "    if len(modified_issues) > 0:\n",
    "        static_predicted_pylint_aggressive.append(instance_id)\n",
    "\n",
    "with open(f'predicted_{method}_results_new.json', 'r') as f:\n",
    "    static_predicted_pyright = json.load(f)\n",
    "static_predicted_all_sa = []\n",
    "for key in static_predicted_pyright:\n",
    "    if 'predicted' in key:\n",
    "        static_predicted_all_sa.extend(static_predicted_pyright[key])\n",
    "static_predicted_results_pyright = list(set(static_predicted_all_sa))\n",
    "static_predicted_pyright_aggressive = []\n",
    "for instance_id, modified_issues in zip(instances, static_predicted_pyright['original_issues']):\n",
    "    if len(modified_issues) > 0:\n",
    "        static_predicted_pyright_aggressive.append(instance_id)\n",
    "\n",
    "if method == 'autocoderover':\n",
    "    method_tmp = 'autocoderover-v20240620-gpt-4o-2024-05-13'\n",
    "    file_path = f'runtime_detection_predicted_{method_tmp}_results_ant_singleshot.jsonl'\n",
    "else:\n",
    "    file_path = f'runtime_detection_predicted_{method}_results_ant_singleshot.jsonl'\n",
    "predicted_results = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        predicted_results.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Set, Tuple, Union\n",
    "from difflib import unified_diff\n",
    "def get_patch(old_file_contents: Dict[str, str], new_file_contents: Dict[str, str], context_lines: int = 3):\n",
    "    \"\"\"Generate a patch from old and new versions of a set of files.\"\"\"\n",
    "    patches = []\n",
    "    for file_name, new_file_content in new_file_contents.items():\n",
    "        # if \"/test/\" in file_name or \"/tests/\" in file_name or \"/testing/\" in file_name:\n",
    "        #     continue\n",
    "        old_file_content = old_file_contents.get(file_name)\n",
    "        diff_gen = unified_diff(\n",
    "            (old_file_content or \"\").splitlines(keepends=True),\n",
    "            new_file_content.splitlines(keepends=True),\n",
    "            fromfile=f\"a/{file_name}\",\n",
    "            tofile=f\"b/{file_name}\",\n",
    "            n=context_lines,\n",
    "        )\n",
    "        # Work around for https://bugs.python.org/issue2142\n",
    "        diff_lines = []\n",
    "        for line in diff_gen:\n",
    "            if line.endswith(\"\\n\"):\n",
    "                diff_lines.append(line)\n",
    "            else:\n",
    "                diff_lines.append(line + \"\\n\")\n",
    "                diff_lines.append(\"\\\\ No newline at end of file\\n\")\n",
    "        diff = \"\".join(diff_lines)\n",
    "        if diff.strip() != \"\":\n",
    "            patches.append((file_name, diff))\n",
    "    patch = \"\".join([p[1] for p in patches])\n",
    "    patch = patch.rstrip(\"\\n\") + \"\\n\"\n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_id = 'django__django-11964'\n",
    "ant_prediction = [item for item in predicted_results if item['instance_id'] == instance_id][0]\n",
    "instance_id = ant_prediction['instance_id']\n",
    "pred = preds[preds['instance_id']==instance_id].iloc[0]\n",
    "record = swebench_lite_df[swebench_lite_df['instance_id']==instance_id].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['repo'] = record['repo']\n",
    "pred['base_commit'] = record['base_commit']\n",
    "pred['patch'] = record['patch']\n",
    "repo_dir = setup_repo(pred)\n",
    "os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_line_numbers = extract_modified_files_and_line_numbers(pred['model_patch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scripts = {}\n",
    "for file in file_line_numbers:\n",
    "    original_script = read_python_script(file)\n",
    "    original_scripts[file] = original_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = pred['model_patch']\n",
    "patch_path = repo_dir / 'predicted.patch'\n",
    "with open(patch_path, 'w') as f:\n",
    "    f.write(patch)\n",
    "apply_patch(patch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {}\n",
    "modified_scripts = {}\n",
    "for file in file_line_numbers:\n",
    "    modified_script = read_python_script(file)\n",
    "    intervals = merge_line_numbers(file_line_numbers[file])\n",
    "    total_lines = len(modified_script.splitlines())\n",
    "    augmented_intervals = augment_intervals(intervals, total_lines)\n",
    "    extracted_code_chunks = extract_code_by_intervals(modified_script.splitlines(), augmented_intervals)\n",
    "    chunks[file] = list(zip(extracted_code_chunks, augmented_intervals))\n",
    "    modified_scripts[file] = modified_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_patch(patch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are an experienced program analyzer who can fix the code according to previously detected runtime errors.\n",
    "\"\"\"\n",
    "EDIT_CODE_CHUNKS_TEMPLATE = \"\"\"\n",
    "Your task is to update the provided code files to prevent the previously detected runtime errors. You will \\\n",
    "be provided with relevant code chunks and identified errors.\n",
    "\n",
    "Begin your response by providing a simple smoke test to test the updated code within <test></test> \\\n",
    "tags. The rest of your response should provide the updated code to prevent the runtime errors, matching \\\n",
    "the exact format of the provided <code> below, including the <code> and <file> tags, and the name \\\n",
    "and start_line attributes. If a code chunk does not need any modification, it can be omitted from \\\n",
    "your response. Each code chunk you update to solve the problem must be rewritten in full, \\\n",
    "including lines that are unchanged. The name and start_line XML attributes in your response should \\\n",
    "always match those in the code below exactly - do not change them. For example, if 100 lines of \\\n",
    "code are passed for a code chunk, but you only modify 5 lines, you must still include the full \\\n",
    "code chunk in your response with the original start_line attribute. If you are able to solve the \\\n",
    "problem, provide <outcome>Complete</outcome> in your response, otherwise provide \\\n",
    "<outcome>Incomplete</outcome>, along with brief feedback and next steps within \\\n",
    "<assessment></assessment> tags.\n",
    "\n",
    "Below is a simple example of a valid response:\n",
    "<example>\n",
    "<smoke_test>\n",
    "from path.to.file import combine_numbers\n",
    "combine_numbers(123, 456)\n",
    "</smoke_test>\n",
    "As requested, in the updated code below, I've rewritten the full chunks provided, even those parts \\\n",
    "that remain unchanged, such as the load_file function.\n",
    "<code>\n",
    "<file name=\"path/to/file1.py\" start_line=\"5\">\n",
    "import numpy as np\n",
    "</file>\n",
    "<file name=\"path/to/file1.py\" start_line=\"23\">\n",
    "def load_file(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def combine_numbers(a, b):\n",
    "    return {{\n",
    "        \"sum\": a + b,\n",
    "        \"difference\": a - b,\n",
    "        \"product\": a * b,\n",
    "        \"quotient\": a / b,\n",
    "        \"geometric_mean\": geometric_mean(a, b),\n",
    "    }}\n",
    "</file>\n",
    "</code>\n",
    "<outcome>Incomplete</outcome>\n",
    "<assessment>\n",
    "Although this patch adds a geometric mean calculation, it does not import the required function to \\\n",
    "the file. The next step is to import the `geometric_mean` function to path/to/file1.py\n",
    "</assessment>\n",
    "</example>\n",
    "\n",
    "Here are the detected runtime errors:\n",
    "<Rumetime errors>\n",
    "{remaining_issues}\n",
    "</Runtime errors>\n",
    "\n",
    "Here are the code chunks:\n",
    "<code>\n",
    "{code}\n",
    "</code>\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_issues = [item.strip() for item in ant_prediction['remaining_issues']]\n",
    "remaining_issues = \"\\n\".join(remaining_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patch(original_scripts, modified_scripts, chunks, remaining_issues):\n",
    "\n",
    "    code_chunks = []\n",
    "    for file in chunks:\n",
    "        for chunk, interval in chunks[file]:\n",
    "            start = interval[0]\n",
    "            code_chunks.append(\n",
    "                f'<file name=\"{file}\" start_line=\"{start}\">\\n{chunk}\\n</file>'\n",
    "            )\n",
    "\n",
    "    prompt = EDIT_CODE_CHUNKS_TEMPLATE.format(\n",
    "            remaining_issues=remaining_issues,\n",
    "            code=\"\\n\\n\".join(code_chunks),\n",
    "        )\n",
    "    max_retries = 10\n",
    "    delay = 20\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = prompt_anthropic(\n",
    "                system=SYSTEM_TEMPLATE,\n",
    "                prompt=prompt,\n",
    "                model_id=OPUS,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "            time.sleep(delay)\n",
    "\n",
    "    chunk_code = extract_tag_list(\"file\", response, remove_leading_newline=True)\n",
    "    chunk_files = extract_tag_attributes(\"file\", \"name\", response)\n",
    "    chunk_starts = extract_tag_attributes(\"file\", \"start_line\", response)\n",
    "\n",
    "    code_chunks_per_file: Dict = {}\n",
    "    for code, file_name, start in zip(chunk_code, chunk_files, chunk_starts):\n",
    "        if not start.isdigit():\n",
    "            logger.warning(\"Malformed start line integer returned by LLM, skipping\")\n",
    "            continue\n",
    "        # Add the chunk to the list of chunks for the file\n",
    "        current_code_chunks_for_file = code_chunks_per_file.get(file_name, [])\n",
    "        current_code_chunks_for_file.append((code, int(start)))\n",
    "        code_chunks_per_file[file_name] = current_code_chunks_for_file\n",
    "\n",
    "    new_code, old_code = {}, {}\n",
    "    for file_name, code_chunks_starts in code_chunks_per_file.items():\n",
    "        # Get matching file (guaranteed by the checks above)\n",
    "        original_code = original_scripts[file_name]\n",
    "        old_code[file_name] = original_code\n",
    "        original_code_lines = original_code.splitlines(keepends=True)\n",
    "\n",
    "        code = modified_scripts[file_name]\n",
    "        code_lines = code.splitlines(keepends=True)\n",
    "        # Sort code chunks in decreasing order to apply to the code from the bottom up\n",
    "        # This prevents earlier chunks from mangling line numbers for later chunks\n",
    "        code_chunks_starts = sorted(code_chunks_starts, key=lambda x: -x[1])\n",
    "        for new_chunk, start in code_chunks_starts:\n",
    "            chunk_matches = [\n",
    "                line_range\n",
    "                for _, line_range in chunks[file_name]\n",
    "                if line_range[0] == start\n",
    "            ]\n",
    "            if not chunk_matches:\n",
    "                logger.error(\"LLM modified chunk start line, failing\")\n",
    "                raise ValueError(\"LLM modified chunk start line, failing\")\n",
    "            end = chunk_matches[0][1]\n",
    "\n",
    "            updated_code: List[str] = []\n",
    "            if start > 0:\n",
    "                updated_code.extend(code_lines[:start])\n",
    "                # updated_code.extend(original_code_lines[:start-1])\n",
    "            updated_code.extend(new_chunk.splitlines(keepends=True))\n",
    "            updated_code.extend(code_lines[end+1:])\n",
    "            # updated_code.extend(original_code_lines[end:])\n",
    "            code_lines = updated_code\n",
    "\n",
    "        new_code[file_name] = \"\".join(code_lines)\n",
    "\n",
    "    patch = get_patch(old_code, new_code)\n",
    "    return patch, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patch_tmp(response, original_scripts, modified_scripts, chunks, remaining_issues):\n",
    "    chunk_code = extract_tag_list(\"file\", response, remove_leading_newline=True)\n",
    "    chunk_files = extract_tag_attributes(\"file\", \"name\", response)\n",
    "    chunk_starts = extract_tag_attributes(\"file\", \"start_line\", response)\n",
    "\n",
    "    code_chunks_per_file: Dict = {}\n",
    "    for code, file_name, start in zip(chunk_code, chunk_files, chunk_starts):\n",
    "        if not start.isdigit():\n",
    "            logger.warning(\"Malformed start line integer returned by LLM, skipping\")\n",
    "            continue\n",
    "        # Add the chunk to the list of chunks for the file\n",
    "        current_code_chunks_for_file = code_chunks_per_file.get(file_name, [])\n",
    "        current_code_chunks_for_file.append((code, int(start)))\n",
    "        code_chunks_per_file[file_name] = current_code_chunks_for_file\n",
    "\n",
    "    new_code, old_code = {}, {}\n",
    "    for file_name, code_chunks_starts in code_chunks_per_file.items():\n",
    "        # Get matching file (guaranteed by the checks above)\n",
    "        original_code = original_scripts[file_name]\n",
    "        old_code[file_name] = original_code\n",
    "        original_code_lines = original_code.splitlines(keepends=True)\n",
    "\n",
    "        code = modified_scripts[file_name]\n",
    "        code_lines = code.splitlines(keepends=True)\n",
    "        # Sort code chunks in decreasing order to apply to the code from the bottom up\n",
    "        # This prevents earlier chunks from mangling line numbers for later chunks\n",
    "        code_chunks_starts = sorted(code_chunks_starts, key=lambda x: -x[1])\n",
    "        for new_chunk, start in code_chunks_starts:\n",
    "            chunk_matches = [\n",
    "                line_range\n",
    "                for _, line_range in chunks[file_name]\n",
    "                if line_range[0] == start\n",
    "            ]\n",
    "\n",
    "            if not chunk_matches:\n",
    "                logger.error(\"LLM modified chunk start line, failing\")\n",
    "                raise ValueError(\"LLM modified chunk start line, failing\")\n",
    "            end = chunk_matches[0][1]\n",
    "\n",
    "            updated_code: List[str] = []\n",
    "            if start > 0:\n",
    "                updated_code.extend(code_lines[:start-1])\n",
    "            updated_code.extend(new_chunk.splitlines(keepends=True))\n",
    "            # updated_code.extend(chunk_matched.splitlines(keepends=True))\n",
    "            updated_code += '\\n'\n",
    "            updated_code.extend(code_lines[end:])\n",
    "            code_lines = updated_code\n",
    "\n",
    "        new_code[file_name] = \"\".join(code_lines)\n",
    "\n",
    "    patch = get_patch(old_code, new_code)\n",
    "    return patch, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_patch, response = generate_patch(original_scripts, modified_scripts, chunks, remaining_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_patch, response = generate_patch_tmp(response, original_scripts, modified_scripts, chunks, remaining_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- a/django/db/models/enums.py\n",
      "+++ b/django/db/models/enums.py\n",
      "@@ -73,3 +73,7 @@\n",
      " \n",
      "     def _generate_next_value_(name, start, count, last_values):\n",
      "         return name\n",
      "+\n",
      "+    def __str__(self):\n",
      "+        return str(self)\n",
      "+\n",
      "--- a/django/db/models/fields/__init__.py\n",
      "+++ b/django/db/models/fields/__init__.py\n",
      "@@ -1010,6 +1010,8 @@\n",
      "     def to_python(self, value):\n",
      "         if isinstance(value, str) or value is None:\n",
      "             return value\n",
      "+        if isinstance(value, self.choices.choice_class):\n",
      "+            return str(value) \n",
      "         return str(value)\n",
      " \n",
      "     def get_prep_value(self, value):\n",
      "@@ -1020,6 +1022,7 @@\n",
      "         # Passing max_length to forms.CharField means that the value's length\n",
      "         # will be validated twice. This is considered acceptable since we want\n",
      "         # the value in the form field (to pass into widget for example).\n",
      "+\n",
      "         defaults = {'max_length': self.max_length}\n",
      "         # TODO: Handle multiple backends with different feature flags.\n",
      "         if self.null and not connection.features.interprets_empty_strings_as_nulls:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(new_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/django/db/models/enums.py b/django/db/models/enums.py\n",
      "index bbe362a6ab..251de832eb 100644\n",
      "--- a/django/db/models/enums.py\n",
      "+++ b/django/db/models/enums.py\n",
      "@@ -73,3 +73,6 @@ class TextChoices(str, Choices):\n",
      " \n",
      "     def _generate_next_value_(name, start, count, last_values):\n",
      "         return name\n",
      "+\n",
      "+    def __str__(self):\n",
      "+        return str(self.value)\n",
      "diff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\n",
      "index d610dc86f0..3801da4268 100644\n",
      "--- a/django/db/models/fields/__init__.py\n",
      "+++ b/django/db/models/fields/__init__.py\n",
      "@@ -1010,6 +1010,8 @@ class CharField(Field):\n",
      "     def to_python(self, value):\n",
      "         if isinstance(value, str) or value is None:\n",
      "             return value\n",
      "+        if isinstance(value, TextChoices):\n",
      "+            return str(value)\n",
      "         return str(value)\n",
      " \n",
      "     def get_prep_value(self, value):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pred['model_patch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_patch_path = repo_dir / 'new_predicted.patch'\n",
    "with open(new_patch_path, 'w') as f:\n",
    "    f.write(patch)\n",
    "apply_patch(new_patch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_patch(new_patch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_patch = \"--- a/astropy/io/fits/fitsrec.py\\n+++ b/astropy/io/fits/fitsrec.py\\n@@ -1261,7 +1261,13 @@\\n \\n         # Replace exponent separator in floating point numbers\\n         if 'D' in format:\\n-            output_field.replace(encode_ascii('E'), encode_ascii('D'))\\n+            if isinstance(output_field, np.ndarray):\\n+                output_field = np.char.replace(output_field, encode_ascii('E'), encode_ascii('D'))\\n+            elif isinstance(output_field, str):\\n+                output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))\\n+            else:\\n+                # Handle other types if necessary\\n+                pass\\n \\n \\n def _get_recarray_field(array, key):\\n@@ -1272,6 +1278,11 @@\\n     \\\"\\\"\\\"\\n \\n     # Numpy >= 1.10.dev recarray no longer returns chararrays for strings\\n+    field = array.field(key)\\n+    if isinstance(field, np.ndarray) and field.dtype.kind in ('S', 'U'):\\n+        return field.view(np.chararray)\\n+    return field\\n+\\n     # This is currently needed for backwards-compatibility and for\\n     # automatic truncation of trailing whitespace\\n     field = np.recarray.field(array, key)\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- a/astropy/io/fits/fitsrec.py\n",
      "+++ b/astropy/io/fits/fitsrec.py\n",
      "@@ -1261,7 +1261,13 @@\n",
      " \n",
      "         # Replace exponent separator in floating point numbers\n",
      "         if 'D' in format:\n",
      "-            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n",
      "+            if isinstance(output_field, np.ndarray):\n",
      "+                output_field = np.char.replace(output_field, encode_ascii('E'), encode_ascii('D'))\n",
      "+            elif isinstance(output_field, str):\n",
      "+                output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n",
      "+            else:\n",
      "+                # Handle other types if necessary\n",
      "+                pass\n",
      " \n",
      " \n",
      " def _get_recarray_field(array, key):\n",
      "@@ -1272,6 +1278,11 @@\n",
      "     \"\"\"\n",
      " \n",
      "     # Numpy >= 1.10.dev recarray no longer returns chararrays for strings\n",
      "+    field = array.field(key)\n",
      "+    if isinstance(field, np.ndarray) and field.dtype.kind in ('S', 'U'):\n",
      "+        return field.view(np.chararray)\n",
      "+    return field\n",
      "+\n",
      "     # This is currently needed for backwards-compatibility and for\n",
      "     # automatic truncation of trailing whitespace\n",
      "     field = np.recarray.field(array, key)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
